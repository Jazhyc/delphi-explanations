{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1096fab",
   "metadata": {},
   "source": [
    "# Explainer Model Performance Analysis\n",
    "\n",
    "This notebook analyzes the performance of different explainer models in generating explanations for SAE latent features. We compare models across multiple metrics including accuracy, F1 scores, token usage, and execution time.\n",
    "\n",
    "## Recent Updates\n",
    "- **Updated to 400 latents**: Changed from 100 to 400 latents to match the experiment configuration\n",
    "- **Fixed directory pattern matching**: Now handles both old (with suffix) and new (without suffix) directory naming conventions\n",
    "- **Simplified loading process**: Now uses functions from `result_analysis.py` to reduce redundancy\n",
    "- **Enhanced model support**: Updated model name mapping for better display names and added GPT-OSS-20B\n",
    "- **Bootstrapped confidence intervals**: Error bars now use 95% CI via bootstrapping (2000 resamples) instead of standard deviation\n",
    "- **Progress bars**: Added TQDM progress bars for bootstrap calculations to show progress\n",
    "- **Better variable naming**: Renamed `accuracy_df` to `performance_df` for clarity\n",
    "\n",
    "## Analysis Overview\n",
    "\n",
    "- **Accuracy Distribution**: Density plots showing accuracy distribution for each model\n",
    "- **Mean Performance**: Bar charts comparing mean accuracy across models\n",
    "- **Token Usage**: Analysis of computational efficiency and resource consumption\n",
    "- **Performance Summary**: Comprehensive comparison tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60643f3",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required libraries, define model name mapping for prettier display names, and set up output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prefix: pythiaST\n",
      "Latents count: 400\n",
      "Compare thinking modes: False\n",
      "Results directory: /home/jeremias/projects/delphi-explanations/results\n",
      "Visualizations output: /home/jeremias/projects/delphi-explanations/results/visualizations\n",
      "Experiments directory: /home/jeremias/projects/delphi-explanations/results/pythiaST/400latents\n",
      "Available experiments (400 latents):\n",
      "  - pythiaST_Llama_3_3_70B_Instruct_quantized_w4a16\n",
      "  - pythiaST_Llama_4_Scout_17B_16E_Instruct_quantized_w4a16\n",
      "  - pythiaST_Meta_Llama_3_1_8B_Instruct_GPTQ_INT4\n",
      "  - pythiaST_Qwen3_14B_quantized_w4a16\n",
      "  - pythiaST_Qwen3_32B_quantized_w4a16\n",
      "  - pythiaST_Qwen3_4B_quantized_w4a16\n",
      "  - pythiaST_gemma_3_12b_it_quantized_w4a16\n",
      "  - pythiaST_gemma_3_27b_it_quantized_w4a16\n",
      "  - pythiaST_gemma_3_4b_it_quantized_w4a16\n",
      "  - pythiaST_gpt_oss_20b\n",
      "  - pythiaST_llama_8b_explainer\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add the parent directory to the path to import delphi modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from delphi.log.result_analysis import (\n",
    "    import_plotly,\n",
    "    load_data,\n",
    "    get_agg_metrics,\n",
    "    add_latent_f1,\n",
    "    compute_confusion,\n",
    "    compute_classification_metrics,\n",
    "    frequency_weighted_f1\n",
    ")\n",
    "\n",
    "# Import plotly for plotting\n",
    "px = import_plotly()\n",
    "\n",
    "# Toggle for whether to run expensive bootstrap resampling\n",
    "ENABLE_BOOTSTRAP = True  # Set to True to run bootstrap resampling (slow)\n",
    "BOOTSTRAP_SAMPLES = 400  # Number of bootstrap resamples when enabled\n",
    "# Confidence level for CI calculations (e.g. 0.95 for 95% CI). Make configurable.\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "# Configuration - Model prefix for experiment naming\n",
    "MODEL_PREFIX = \"pythiaST\"  # Change this to match your base model (e.g., \"pythia\", \"gemma\", etc.)\n",
    "LATENTS_COUNT = 400  # Number of latents used in experiments\n",
    "COMPARE_THINKING_MODES = False  # Set to True to compare both thinking and non-thinking modes (Currently broken if set to True)\n",
    "\n",
    "# Define model name mapping for prettier display - updated to match actual directory names\n",
    "MODEL_NAME_MAPPING = {\n",
    "    \"gemma_3_4b_it_quantized_w4a16\": \"Gemma-3-4B-IT\",\n",
    "    \"Qwen3_4B_quantized_w4a16\": \"Qwen3-4B\", \n",
    "    \"gemma_3_12b_it_quantized_w4a16\": \"Gemma-3-12B-IT\",\n",
    "    \"gemma_3_27b_it_quantized_w4a16\": \"Gemma-3-27B-IT\",\n",
    "    \"Qwen3_14B_quantized_w4a16\": \"Qwen3-14B\",\n",
    "    \"Qwen3_32B_quantized_w4a16\": \"Qwen3-32B\",\n",
    "    \"Qwen3_235B_A22B_GPTQ_Int4\": \"Qwen3-235B\",\n",
    "    \"Llama_3_3_70B_Instruct_quantized_w4a16\": \"Llama-3.3-70B-Instruct\",\n",
    "    \"Llama_3_1_70B_Instruct_NVFP4\": \"Llama-3.1-70B-Instruct\",\n",
    "    \"Llama_4_Scout_17B_16E_Instruct_quantized_w4a16\": \"Llama-4-Scout-17B\",\n",
    "    \"Llama_4_Maverick_17B_128E_Instruct_quantized_w4a16\": \"Llama-4-Maverick-17B\",\n",
    "    \"llama_8b_explainer\": \"Transluce-Llama-8B (Qwen 32B Scorer)\",\n",
    "    \"Meta_Llama_3_1_8B_Instruct_GPTQ_INT4\": \"Llama-8B (Qwen 32B Scorer)\",\n",
    "    \"gpt_oss_20b\": \"GPT-OSS-20B\"\n",
    "}\n",
    "\n",
    "def load_model_results(results_dir: Path, model_mapping: dict, model_prefix: str, latents_count: int, compare_modes: bool = False):\n",
    "    \"\"\"Load results and statistics for all explainer models using functions from result_analysis.py.\"\"\"\n",
    "    model_results = {}\n",
    "    model_stats = {}\n",
    "\n",
    "    # Base directory for experiments with this latent count\n",
    "    experiments_base = results_dir / model_prefix / f\"{latents_count}latents\"\n",
    "\n",
    "    if not experiments_base.exists():\n",
    "        print(f\"Warning: Experiments directory not found: {experiments_base}\")\n",
    "        return model_results, model_stats\n",
    "\n",
    "    # Pattern for experiment directories (exclude thinking mode and hide directories)\n",
    "    # Handle both naming conventions: with and without latents suffix\n",
    "    pattern_with_suffix = f\"{model_prefix}_*_{latents_count}latents\"\n",
    "    pattern_without_suffix = f\"{model_prefix}_*\"\n",
    "    \n",
    "    all_exp_dirs = []\n",
    "    # First try directories with latents suffix (older format)\n",
    "    for exp_dir in experiments_base.glob(pattern_with_suffix):\n",
    "        if not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name):\n",
    "            all_exp_dirs.append(exp_dir)\n",
    "    \n",
    "    # Then try directories without latents suffix (newer format)\n",
    "    if not all_exp_dirs:\n",
    "        for exp_dir in experiments_base.glob(pattern_without_suffix):\n",
    "            if (not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name) and \n",
    "                exp_dir.name != \"cache\"):  # Exclude cache directory\n",
    "                all_exp_dirs.append(exp_dir)\n",
    "\n",
    "    for exp_dir in all_exp_dirs:\n",
    "        # Skip directories with \"thinking\" or \"hide\" in the name\n",
    "        if \"thinking\" in exp_dir.name or \"hide\" in exp_dir.name:\n",
    "            continue\n",
    "\n",
    "        # Extract model name from directory: handle both naming conventions\n",
    "        dir_name = exp_dir.name\n",
    "        if dir_name.endswith(f\"_{latents_count}latents\"):\n",
    "            # Old format: pythiaST_model_400latents\n",
    "            model_key = dir_name.replace(f\"{model_prefix}_\", \"\").replace(f\"_{latents_count}latents\", \"\")\n",
    "        else:\n",
    "            # New format: pythiaST_model\n",
    "            model_key = dir_name.replace(f\"{model_prefix}_\", \"\")\n",
    "        \n",
    "        base_display_name = model_mapping.get(model_key, model_key)\n",
    "        display_name = base_display_name\n",
    "\n",
    "        scores_path = exp_dir / \"scores\"\n",
    "        if scores_path.exists():\n",
    "            # Load scoring results using functions from result_analysis.py\n",
    "            try:\n",
    "                # All experiments now use the shared cache\n",
    "                cache_dir_name = \"cache\"\n",
    "                latents_path = experiments_base.parent / cache_dir_name / \"latents\"\n",
    "                if not latents_path.exists():\n",
    "                    latents_path = exp_dir / \"latents\"  # Fallback to local latents\n",
    "\n",
    "                if latents_path.exists():\n",
    "                    # Extract module names from the actual files\n",
    "                    sample_score_dir = next(scores_path.iterdir())\n",
    "                    sample_files = list(sample_score_dir.glob(\"*.txt\"))\n",
    "                    if sample_files:\n",
    "                        # Extract module name from filename pattern (e.g., \"layers.32_latent0.txt\" -> \"layers.32\")\n",
    "                        sample_filename = sample_files[0].stem\n",
    "                        module_name = sample_filename.split('_latent')[0]\n",
    "                        modules = [module_name]\n",
    "                    else:\n",
    "                        print(f\"No score files found in {sample_score_dir}\")\n",
    "                        continue\n",
    "\n",
    "                    # Use load_data from result_analysis.py\n",
    "                    latent_df, counts = load_data(scores_path, latents_path, modules)\n",
    "\n",
    "                    if latent_df.empty:\n",
    "                        print(f\"No data found for {display_name}\")\n",
    "                        continue\n",
    "\n",
    "                    # Use add_latent_f1 and get_agg_metrics from result_analysis.py\n",
    "                    latent_df = add_latent_f1(latent_df)\n",
    "                    processed_df = get_agg_metrics(latent_df, counts)\n",
    "\n",
    "                    model_results[display_name] = {\n",
    "                        'latent_df': latent_df,\n",
    "                        'processed_df': processed_df,\n",
    "                        'counts': counts\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"Latents path not found for {display_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading results for {model_key}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Load explainer statistics\n",
    "        stats_file = exp_dir / \"explainer_stats.json\"\n",
    "        if stats_file.exists():\n",
    "            try:\n",
    "                with open(stats_file, 'r') as f:\n",
    "                    stats = json.load(f)\n",
    "                    model_stats[display_name] = stats\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading stats for {model_key}: {e}\")\n",
    "                model_stats[display_name] = None\n",
    "        else:\n",
    "            model_stats[display_name] = None\n",
    "\n",
    "    return model_results, model_stats\n",
    "\n",
    "# Set up directories\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "visualizations_dir = results_dir / \"visualizations\"\n",
    "visualizations_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Model prefix: {MODEL_PREFIX}\")\n",
    "print(f\"Latents count: {LATENTS_COUNT}\")\n",
    "print(f\"Compare thinking modes: {COMPARE_THINKING_MODES}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Visualizations output: {visualizations_dir}\")\n",
    "\n",
    "# Check if the experiments directory exists\n",
    "experiments_dir = results_dir / MODEL_PREFIX / f\"{LATENTS_COUNT}latents\"\n",
    "print(f\"Experiments directory: {experiments_dir}\")\n",
    "\n",
    "if experiments_dir.exists():\n",
    "    print(f\"Available experiments ({LATENTS_COUNT} latents):\")\n",
    "    # Handle both naming conventions: with and without latents suffix\n",
    "    pattern_with_suffix = f\"{MODEL_PREFIX}_*_{LATENTS_COUNT}latents\"\n",
    "    pattern_without_suffix = f\"{MODEL_PREFIX}_*\"\n",
    "    \n",
    "    all_exp_dirs = []\n",
    "    # First try directories with latents suffix (older format)\n",
    "    for exp_dir in sorted(experiments_dir.glob(pattern_with_suffix)):\n",
    "        if not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name):\n",
    "            all_exp_dirs.append(exp_dir)\n",
    "    \n",
    "    # Then try directories without latents suffix (newer format)\n",
    "    if not all_exp_dirs:\n",
    "        for exp_dir in sorted(experiments_dir.glob(pattern_without_suffix)):\n",
    "            if (not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name) and \n",
    "                exp_dir.name != \"cache\"):  # Exclude cache directory\n",
    "                all_exp_dirs.append(exp_dir)\n",
    "    \n",
    "    for exp_dir in all_exp_dirs:\n",
    "        print(f\"  - {exp_dir.name}\")\n",
    "else:\n",
    "    print(f\"Warning: Experiments directory not found: {experiments_dir}\")\n",
    "    print(\"Make sure experiments have been run with the specified latent count.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a672b",
   "metadata": {},
   "source": [
    "## 2. Load Model Results\n",
    "\n",
    "Load explanation comparison results from all models and extract performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44990708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded results for 11 models:\n",
      "  - Qwen3-4B\n",
      "  - Gemma-3-4B-IT\n",
      "  - Gemma-3-12B-IT\n",
      "  - Gemma-3-27B-IT\n",
      "  - Qwen3-14B\n",
      "  - Qwen3-32B\n",
      "  - Llama-3.3-70B-Instruct\n",
      "  - GPT-OSS-20B\n",
      "  - Llama-8B (Qwen 32B Scorer)\n",
      "  - Llama-4-Scout-17B\n",
      "  - Transluce-Llama-8B (Qwen 32B Scorer)\n",
      "\n",
      "Token usage statistics available for 11 models:\n",
      "  - Qwen3-4B: ['DefaultExplainer']\n",
      "  - Gemma-3-4B-IT: ['DefaultExplainer']\n",
      "  - Gemma-3-12B-IT: ['DefaultExplainer']\n",
      "  - Gemma-3-27B-IT: ['DefaultExplainer']\n",
      "  - Qwen3-14B: ['DefaultExplainer']\n",
      "  - Qwen3-32B: ['DefaultExplainer']\n",
      "  - Llama-3.3-70B-Instruct: ['DefaultExplainer']\n",
      "  - GPT-OSS-20B: ['DefaultExplainer']\n",
      "  - Llama-8B (Qwen 32B Scorer): ['DefaultExplainer']\n",
      "  - Llama-4-Scout-17B: ['DefaultExplainer']\n",
      "  - Transluce-Llama-8B (Qwen 32B Scorer): ['DefaultExplainer']\n",
      "\n",
      "Sample metrics from Qwen3-4B:\n",
      "  score_type  accuracy  f1_score  precision  recall\n",
      "0  detection     0.757     0.755      0.763   0.747\n",
      "1       fuzz     0.818     0.827      0.787   0.871\n"
     ]
    }
   ],
   "source": [
    "# Load all model results and statistics\n",
    "print(\"Loading model results...\")\n",
    "model_results, model_stats = load_model_results(results_dir, MODEL_NAME_MAPPING, MODEL_PREFIX, LATENTS_COUNT, COMPARE_THINKING_MODES)\n",
    "\n",
    "print(f\"\\nLoaded results for {len(model_results)} models:\")\n",
    "for model_name in model_results.keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "print(f\"\\nToken usage statistics available for {len([k for k, v in model_stats.items() if v is not None])} models:\")\n",
    "for model_name, stats in model_stats.items():\n",
    "    if stats:\n",
    "        print(f\"  - {model_name}: {list(stats.keys())}\")\n",
    "    else:\n",
    "        print(f\"  - {model_name}: No stats available\")\n",
    "\n",
    "# Display sample metrics for the first model\n",
    "if model_results:\n",
    "    sample_model = list(model_results.keys())[0]\n",
    "    sample_data = model_results[sample_model]['processed_df']\n",
    "    print(f\"\\nSample metrics from {sample_model}:\")\n",
    "    print(sample_data[['score_type', 'accuracy', 'f1_score', 'precision', 'recall']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a674b50",
   "metadata": {},
   "source": [
    "## 3. Generate Mean F1 Score Bar Charts\n",
    "\n",
    "Create bar charts displaying mean F1 scores with **bootstrapped 95% confidence intervals** (2000 resamples) for each model and score type. This provides more robust uncertainty estimates compared to standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0916e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing frequency-weighted F1 score bar charts (bootstrapping=True)...\n",
      "Computing error bars for score type: detection (bootstrap=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b78387f69f94d6195aa9c815b0bbb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ec5e949f34b3c85e5f17bb1e79e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872a5f4b952749ae863c5cd9cf7928fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1db1d9892d4f63a80cc4381213d085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea6cdf891724b3a839c703881d17c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dd1e2841db42bb8532bc97829aa1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25df5a8a4e24d2c86c87ddc86a38cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bab4183c83430a9c377d5b28313a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b0226ed528492db3d9d4c42eb5570b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cadc2e5c6b343afb806916adb202c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c9ea70b58f47c2bb6b488c01bb7824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing error bars for score type: fuzz (bootstrap=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70957d5e2b24816bbf5e4c35e0291de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab46687d38464476bf76e687f93436e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6338940d6a294959bca27ec6660a2963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e9bfdec6584e76a5c3620d86e3e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4af16380a67471cb805e986d5f784fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1fcdead43a471da2f24c57d26349d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61af0bb478304244b50e7ba2633c190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7d1bbf1b6c422bad39a40133be0a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e2f602c1074c1ab3bdf150e4d02525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61960ab16e34b92a2bb9a83839e4896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f537588942438284efc3d92b949482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast Bootstrapping (50):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "error_y": {
          "array": {
           "bdata": "YFCknqTQnj9Ab5s5lFiaP6BNPvY/75k/QG79f5Qsnj8APyw5qJSYP1BEAluHSKA/8CvfYvSYoT9gxGYn1bCcP8CUeqTnnZw/APAlXMOolz8IiFnBcKWxPw==",
           "dtype": "f8"
          },
          "arrayminus": {
           "bdata": "wB2vUuCflj/gSskuHfGUP2A0al3h7ZM/AJ/tJX50mD+AaccDHEWMP5Cy0Suqsak/CAFX0uuHsT8AsM2+F0WYP4AktIIUmqU/QLN1SDEhmT9kAYzE7Ci1Pw==",
           "dtype": "f8"
          },
          "symmetric": false,
          "type": "data"
         },
         "hovertemplate": "model=%{x}<br>frequency_weighted_f1=%{text}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "text": {
          "bdata": "AAAAYFTg5T8AAABgKdLlPwAAAIBUSeU/AAAAoN0t5T8AAACg4+rkPwAAAIDjeeQ/AAAAAIZB5D8AAABASBjkPwAAAMCLjuM/AAAAIMEl4j8AAAAALPPbPw==",
          "dtype": "f8"
         },
         "textposition": "outside",
         "texttemplate": "%{text:.3f}",
         "type": "bar",
         "x": [
          "Gemma-3-27B-IT",
          "Qwen3-32B",
          "Gemma-3-12B-IT",
          "Qwen3-14B",
          "Qwen3-4B",
          "Llama-4-Scout-17B",
          "Llama-3.3-70B-Instruct",
          "Llama-8B (Qwen 32B Scorer)",
          "Transluce-Llama-8B (Qwen 32B Scorer)",
          "Gemma-3-4B-IT",
          "GPT-OSS-20B"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAYFTg5T8AAABgKdLlPwAAAIBUSeU/AAAAoN0t5T8AAACg4+rkPwAAAIDjeeQ/AAAAAIZB5D8AAABASBjkPwAAAMCLjuM/AAAAIMEl4j8AAAAALPPbPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "height": 500,
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency-Weighted F1 Score by Model - Detection (95% CI)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickangle": 45,
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1
         ],
         "title": {
          "text": "Frequency-Weighted F1 Score (95% CI)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303689/3120134952.py:222: DeprecationWarning:\n",
      "\n",
      "\n",
      "Support for Kaleido versions less than 1.0.0 is deprecated and will be removed after September 2025.\n",
      "Please upgrade Kaleido to version 1.0.0 or greater (`pip install 'kaleido>=1.0.0'` or `pip install 'plotly[kaleido]'`).\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Frequency-Weighted F1 bar chart: /home/jeremias/projects/delphi-explanations/results/visualizations/freq_weighted_f1_bar_detection.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303689/3120134952.py:223: DeprecationWarning:\n",
      "\n",
      "\n",
      "Support for Kaleido versions less than 1.0.0 is deprecated and will be removed after September 2025.\n",
      "Please upgrade Kaleido to version 1.0.0 or greater (`pip install 'kaleido>=1.0.0'` or `pip install 'plotly[kaleido]'`).\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "error_y": {
          "array": {
           "bdata": "0ICJbRd2sT8g8RAyrqGeP7AKZmDLQKY/sMZlOQFmsD/AnmNFfxClP4CTDKXGjrc/AFDxWIEEjz+QGU69CBuwP1ieohSGFLw/AJCnqbZwsj8AIccTqWO0Pw==",
           "dtype": "f8"
          },
          "arrayminus": {
           "bdata": "AC+BSgKcpj+gs402yR2ZPwC0nGsWwqE/8ADK6BDbqT/wAtHP6iOlP7huivG1sLA/QJXPZFpnlj8AKQzZo1unP1CrQk1uDrI/EAnPZ9Vopz/8GAXL0IWuPw==",
           "dtype": "f8"
          },
          "symmetric": false,
          "type": "data"
         },
         "hovertemplate": "model=%{x}<br>frequency_weighted_f1=%{text}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "text": {
          "bdata": "AAAAgMU26D8AAACAz+nnPwAAAKCe/OY/AAAAoLTD5j8AAAAAU6LmPwAAAIAQZ+U/AAAAoOUC5D8AAAAg58jjPwAAAIB/7+I/AAAAwAPF4j8AAABggJ7TPw==",
          "dtype": "f8"
         },
         "textposition": "outside",
         "texttemplate": "%{text:.3f}",
         "type": "bar",
         "x": [
          "Qwen3-32B",
          "Gemma-3-27B-IT",
          "Qwen3-4B",
          "Transluce-Llama-8B (Qwen 32B Scorer)",
          "Llama-4-Scout-17B",
          "Llama-3.3-70B-Instruct",
          "Gemma-3-4B-IT",
          "Gemma-3-12B-IT",
          "Qwen3-14B",
          "Llama-8B (Qwen 32B Scorer)",
          "GPT-OSS-20B"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAgMU26D8AAACAz+nnPwAAAKCe/OY/AAAAoLTD5j8AAAAAU6LmPwAAAIAQZ+U/AAAAoOUC5D8AAAAg58jjPwAAAIB/7+I/AAAAwAPF4j8AAABggJ7TPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "height": 500,
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Frequency-Weighted F1 Score by Model - Fuzz (95% CI)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "tickangle": 45,
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1
         ],
         "title": {
          "text": "Frequency-Weighted F1 Score (95% CI)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303689/3120134952.py:222: DeprecationWarning:\n",
      "\n",
      "\n",
      "Support for Kaleido versions less than 1.0.0 is deprecated and will be removed after September 2025.\n",
      "Please upgrade Kaleido to version 1.0.0 or greater (`pip install 'kaleido>=1.0.0'` or `pip install 'plotly[kaleido]'`).\n",
      "\n",
      "\n",
      "/tmp/ipykernel_303689/3120134952.py:223: DeprecationWarning:\n",
      "\n",
      "\n",
      "Support for Kaleido versions less than 1.0.0 is deprecated and will be removed after September 2025.\n",
      "Please upgrade Kaleido to version 1.0.0 or greater (`pip install 'kaleido>=1.0.0'` or `pip install 'plotly[kaleido]'`).\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Frequency-Weighted F1 bar chart: /home/jeremias/projects/delphi-explanations/results/visualizations/freq_weighted_f1_bar_fuzz.pdf\n",
      "\n",
      "Bar charts saved to /home/jeremias/projects/delphi-explanations/results/visualizations\n"
     ]
    }
   ],
   "source": [
    "# Generate inline bar charts for frequency-weighted F1 scores with optional bootstrapped CI error bars and save to files\n",
    "print(\"Preparing frequency-weighted F1 score bar charts (bootstrapping={})...\".format(ENABLE_BOOTSTRAP))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Helper: per-module F1s (unchanged)\n",
    "def compute_per_module_f1s(score_subset, counts):\n",
    "    \"\"\"Return list of per-module frequency-weighted F1 values computed from the given score_subset (no resampling).\n",
    "    Each module contributes a single weighted F1 computed from its latents in score_subset.\"\"\"\n",
    "    rows = []\n",
    "    for (module, latent_idx), grp in score_subset.groupby([\"module\", \"latent_idx\"]):\n",
    "        if module in counts and latent_idx < len(counts[module]):\n",
    "            f1 = compute_classification_metrics(compute_confusion(grp))[\"f1_score\"]\n",
    "            fire = counts[module][latent_idx].item()\n",
    "            rows.append({\"module\": module, \"latent_idx\": latent_idx, \"f1_score\": f1, \"firing_count\": fire})\n",
    "    if not rows:\n",
    "        return []\n",
    "    df = pd.DataFrame(rows)\n",
    "    per_module_vals = []\n",
    "    for module in df['module'].unique():\n",
    "        mdf = df[df['module'] == module]\n",
    "        firing_weights = counts[module][mdf['latent_idx']].float()\n",
    "        total_weight = firing_weights.sum()\n",
    "        if total_weight > 0:\n",
    "            f1_tensor = torch.as_tensor(mdf['f1_score'].values, dtype=torch.float32)\n",
    "            module_f1 = (f1_tensor * firing_weights).sum() / firing_weights.sum()\n",
    "            per_module_vals.append(module_f1.item())\n",
    "    return per_module_vals\n",
    "\n",
    "# Helper: weighted percentile (unchanged)\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    \"\"\"Compute weighted percentiles for 1D arrays of values and corresponding non-negative weights.\"\"\"\n",
    "    if len(values) == 0:\n",
    "        return [np.nan for _ in np.atleast_1d(percentiles)]\n",
    "    vals = np.asarray(values, dtype=float)\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    if w.sum() == 0:\n",
    "        # Fall back to unweighted percentiles\n",
    "        return np.percentile(vals, percentiles)\n",
    "    # sort by value\n",
    "    order = np.argsort(vals)\n",
    "    vals = vals[order]\n",
    "    w = w[order]\n",
    "    cumsum = np.cumsum(w)\n",
    "    total = cumsum[-1]\n",
    "    # cumulative fraction positions of each value\n",
    "    cumfrac = (cumsum - 0.5 * w) / total\n",
    "    out = []\n",
    "    for p in np.atleast_1d(percentiles) / 100.0:\n",
    "        # find the first index where cumfrac >= p\n",
    "        idx = np.searchsorted(cumfrac, p, side='left')\n",
    "        if idx >= len(vals):\n",
    "            out.append(vals[-1])\n",
    "        else:\n",
    "            out.append(vals[idx])\n",
    "    return out if len(out) > 1 else out[0]\n",
    "\n",
    "# Helper: compute CI errors from samples\n",
    "def compute_ci_from_samples(samples, freq_weighted_f1, confidence):\n",
    "    if not samples:\n",
    "        return 0.0, 0.0\n",
    "    lower_pct = (1.0 - confidence) / 2.0 * 100.0\n",
    "    upper_pct = (1.0 + confidence) / 2.0 * 100.0\n",
    "    lower_ci, upper_ci = np.percentile(samples, [lower_pct, upper_pct])\n",
    "    return float(freq_weighted_f1 - lower_ci), float(upper_ci - freq_weighted_f1)\n",
    "\n",
    "# Optimized bootstrap: avoid DataFrame creation in loop using multinomial + np.bincount\n",
    "def run_bootstrap_single_thread_optimized(score_subset, counts, n_boot, confidence):\n",
    "    \"\"\"Optimized bootstrap using multinomial sampling on precomputed latents.\n",
    "\n",
    "    Strategy:\n",
    "    - Precompute per-latent F1 and firing weights and module ids\n",
    "    - For each bootstrap iteration generate multinomial counts over latents\n",
    "    - Use np.bincount to aggregate weighted sums per module efficiently\n",
    "    \"\"\"\n",
    "    # Pre-compute grouped stats once\n",
    "    grouped = list(score_subset.groupby([\"module\", \"latent_idx\"]))\n",
    "\n",
    "    latent_f1s = []\n",
    "    latent_weights = []\n",
    "    module_names = []\n",
    "\n",
    "    for (module, latent_idx), grp in grouped:\n",
    "        if module in counts and latent_idx < len(counts[module]):\n",
    "            f1 = compute_classification_metrics(compute_confusion(grp))[\"f1_score\"]\n",
    "            fire_count = counts[module][latent_idx].item()\n",
    "            latent_f1s.append(float(f1))\n",
    "            latent_weights.append(float(fire_count))\n",
    "            module_names.append(module)\n",
    "\n",
    "    if not latent_f1s:\n",
    "        return []\n",
    "\n",
    "    latent_f1s = np.asarray(latent_f1s, dtype=np.float64)\n",
    "    latent_weights = np.asarray(latent_weights, dtype=np.float64)\n",
    "    unique_modules, module_ids = np.unique(module_names, return_inverse=True)\n",
    "    num_modules = len(unique_modules)\n",
    "    num_latents = len(latent_f1s)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # Generate multinomial samples: draw counts across latents\n",
    "    # Use equal probability across latents (resampling latents uniformly)\n",
    "    # sum of each row will be num_latents (you can change this to len(score_subset) if desired)\n",
    "    try:\n",
    "        resampled_counts = np.random.multinomial(num_latents, [1.0 / num_latents] * num_latents, size=n_boot)\n",
    "    except Exception:\n",
    "        # Fallback: generate per-iteration to avoid large allocation\n",
    "        resampled_counts = None\n",
    "\n",
    "    for i in tqdm(range(n_boot), desc=f\"Fast Bootstrapping ({n_boot})\", leave=False):\n",
    "        if resampled_counts is None:\n",
    "            boot_counts = np.random.multinomial(num_latents, [1.0 / num_latents] * num_latents)\n",
    "        else:\n",
    "            boot_counts = resampled_counts[i]\n",
    "\n",
    "        # Numerator: sum of f1 * latent_weight * boot_count per module\n",
    "        numerator = np.bincount(module_ids, weights=latent_f1s * latent_weights * boot_counts, minlength=num_modules)\n",
    "        # Denominator: sum of latent_weight * boot_count per module\n",
    "        denominator = np.bincount(module_ids, weights=latent_weights * boot_counts, minlength=num_modules)\n",
    "\n",
    "        # per-module f1 (avoid division by zero)\n",
    "        per_module_f1s = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator != 0)\n",
    "        samples.append(float(per_module_f1s.mean()))\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Main: compute_error_bars simplified to use the optimized bootstrap\n",
    "def compute_error_bars(model_results, score_type, enable_bootstrap: bool = ENABLE_BOOTSTRAP, n_boot: int = BOOTSTRAP_SAMPLES, confidence: float = CONFIDENCE_LEVEL):\n",
    "    \"\"\"Compute frequency-weighted F1 and CI errors for every model for a given score_type.\n",
    "\n",
    "    Returns a DataFrame with columns: model, frequency_weighted_f1, ci_lower_error, ci_upper_error\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model_name, data in model_results.items():\n",
    "        processed_df = data['processed_df']\n",
    "        score_row = processed_df[processed_df['score_type'] == score_type]\n",
    "        if len(score_row) == 0:\n",
    "            continue\n",
    "        freq_weighted_f1 = score_row['weighted_f1'].iloc[0] if 'weighted_f1' in score_row.columns else None\n",
    "        latent_df = data['latent_df']\n",
    "        counts = data['counts']\n",
    "        score_subset = latent_df[latent_df['score_type'] == score_type]\n",
    "        if len(score_subset) == 0 or counts is None or freq_weighted_f1 is None:\n",
    "            rows.append({'model': model_name, 'frequency_weighted_f1': freq_weighted_f1, 'ci_lower_error': 0.0, 'ci_upper_error': 0.0})\n",
    "            continue\n",
    "\n",
    "        if enable_bootstrap:\n",
    "            # Use optimized multinomial-based bootstrap\n",
    "            samples = run_bootstrap_single_thread_optimized(score_subset, counts, n_boot, confidence)\n",
    "            ci_lower_error, ci_upper_error = compute_ci_from_samples(samples, freq_weighted_f1, confidence)\n",
    "        else:\n",
    "            # Fast path: compute weighted percentiles across per-latent F1s using firing_count as weight\n",
    "            vals = []\n",
    "            wts = []\n",
    "            for (module, latent_idx), grp in score_subset.groupby([\"module\", \"latent_idx\"]):\n",
    "                if module in counts and latent_idx < len(counts[module]):\n",
    "                    f1 = compute_classification_metrics(compute_confusion(grp))[\"f1_score\"]\n",
    "                    fire = counts[module][latent_idx].item()\n",
    "                    vals.append(float(f1))\n",
    "                    wts.append(float(fire))\n",
    "            if len(vals) == 0:\n",
    "                ci_lower_error = 0.0\n",
    "                ci_upper_error = 0.0\n",
    "            else:\n",
    "                lower_pct = (1.0 - confidence) / 2.0 * 100.0\n",
    "                upper_pct = (1.0 + confidence) / 2.0 * 100.0\n",
    "                lower_ci, upper_ci = weighted_percentile(np.array(vals), np.array(wts), [lower_pct, upper_pct])\n",
    "                lower_ci = float(lower_ci)\n",
    "                upper_ci = float(upper_ci)\n",
    "                ci_lower_error = float(freq_weighted_f1 - lower_ci)\n",
    "                ci_upper_error = float(upper_ci - freq_weighted_f1)\n",
    "\n",
    "        rows.append({'model': model_name, 'frequency_weighted_f1': float(freq_weighted_f1), 'ci_lower_error': ci_lower_error, 'ci_upper_error': ci_upper_error})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example: compute error bars for each score_type and save/use for plotting\n",
    "all_score_types = set()\n",
    "for data in model_results.values():\n",
    "    all_score_types.update(list(data['processed_df']['score_type'].unique()))\n",
    "all_score_types = sorted(list(all_score_types))\n",
    "\n",
    "# Precompute error bar tables for reuse in other plots\n",
    "error_bar_tables = {}\n",
    "for st in all_score_types:\n",
    "    print(f\"Computing error bars for score type: {st} (bootstrap={ENABLE_BOOTSTRAP})\")\n",
    "    error_bar_tables[st] = compute_error_bars(model_results, st, enable_bootstrap=ENABLE_BOOTSTRAP, n_boot=BOOTSTRAP_SAMPLES, confidence=CONFIDENCE_LEVEL)\n",
    "\n",
    "# Now create bar charts using precomputed error bars\n",
    "for score_type, error_df in error_bar_tables.items():\n",
    "    if error_df.empty:\n",
    "        continue\n",
    "    error_df = error_df.sort_values('frequency_weighted_f1', ascending=False)\n",
    "    fig = px.bar(\n",
    "        error_df,\n",
    "        x='model',\n",
    "        y='frequency_weighted_f1',\n",
    "        title=f'Frequency-Weighted F1 Score by Model - {score_type.title()} ({int(CONFIDENCE_LEVEL*100)}% CI)',\n",
    "        text='frequency_weighted_f1'\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=error_df['ci_upper_error'],\n",
    "            arrayminus=error_df['ci_lower_error']\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis_range=[0, 1],\n",
    "        xaxis_title=\"Model\",\n",
    "        yaxis_title=f\"Frequency-Weighted F1 Score ({int(CONFIDENCE_LEVEL*100)}% CI)\",\n",
    "        xaxis={'tickangle': 45},\n",
    "        height=500\n",
    "    )\n",
    "    fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "    fig.show()\n",
    "    output_file_pdf = visualizations_dir / f\"freq_weighted_f1_bar_{score_type}.pdf\"\n",
    "    output_file_png = visualizations_dir / f\"freq_weighted_f1_bar_{score_type}.png\"\n",
    "    fig.write_image(str(output_file_pdf))\n",
    "    fig.write_image(str(output_file_png))\n",
    "    print(f\"Saved Frequency-Weighted F1 bar chart: {output_file_pdf}\")\n",
    "\n",
    "print(\"\\nBar charts saved to\", visualizations_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1c475",
   "metadata": {},
   "source": [
    "## 5. Performance Summary\n",
    "\n",
    "Display a summary table of F1 scores for each model and score type, including mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f55b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive performance summary...\n",
      "\n",
      "Model Performance Summary (Frequency-Weighted F1 Focus):\n",
      "============================================================\n",
      "                                      frequency_weighted_f1  accuracy  \\\n",
      "model                                                                   \n",
      "Qwen3-32B                                             0.719     0.845   \n",
      "Gemma-3-27B-IT                                        0.715     0.833   \n",
      "Qwen3-4B                                              0.686     0.787   \n",
      "Llama-4-Scout-17B                                     0.674     0.813   \n",
      "Transluce-Llama-8B (Qwen 32B Scorer)                  0.661     0.846   \n",
      "Llama-3.3-70B-Instruct                                0.651     0.842   \n",
      "Gemma-3-12B-IT                                        0.642     0.788   \n",
      "Qwen3-14B                                             0.627     0.813   \n",
      "Llama-8B (Qwen 32B Scorer)                            0.607     0.775   \n",
      "Gemma-3-4B-IT                                         0.596     0.584   \n",
      "GPT-OSS-20B                                           0.372     0.669   \n",
      "\n",
      "                                      precision  recall  \n",
      "model                                                    \n",
      "Qwen3-32B                                 0.855   0.831  \n",
      "Gemma-3-27B-IT                            0.817   0.856  \n",
      "Qwen3-4B                                  0.775   0.809  \n",
      "Llama-4-Scout-17B                         0.826   0.793  \n",
      "Transluce-Llama-8B (Qwen 32B Scorer)      0.898   0.782  \n",
      "Llama-3.3-70B-Instruct                    0.870   0.806  \n",
      "Gemma-3-12B-IT                            0.816   0.750  \n",
      "Qwen3-14B                                 0.877   0.731  \n",
      "Llama-8B (Qwen 32B Scorer)                0.808   0.735  \n",
      "Gemma-3-4B-IT                             0.567   0.733  \n",
      "GPT-OSS-20B                               0.656   0.755  \n",
      "\n",
      "Summary saved to: /home/jeremias/projects/delphi-explanations/results/visualizations/model_frequency_weighted_f1_summary.csv\n",
      "\n",
      "Best Performing Models by Category:\n",
      "==================================================\n",
      "Highest Frequency-Weighted F1: Qwen3-32B (0.719)\n",
      "Highest Accuracy: Transluce-Llama-8B (Qwen 32B Scorer) (0.846)\n",
      "Highest Precision: Transluce-Llama-8B (Qwen 32B Scorer) (0.898)\n",
      "Highest Recall: Gemma-3-27B-IT (0.856)\n",
      "\n",
      "All visualizations and summaries saved to: /home/jeremias/projects/delphi-explanations/results/visualizations\n",
      "\n",
      "Generated files:\n",
      "  - accuracy_bar_detection.pdf\n",
      "  - accuracy_bar_detection.png\n",
      "  - accuracy_bar_fuzz.pdf\n",
      "  - accuracy_bar_fuzz.png\n",
      "  - accuracy_density_detection.pdf\n",
      "  - accuracy_density_detection.png\n",
      "  - accuracy_density_fuzz.pdf\n",
      "  - accuracy_density_fuzz.png\n",
      "  - accuracy_density_line_detection.pdf\n",
      "  - accuracy_density_line_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_detection.pdf\n",
      "  - freq_weighted_f1_bar_detection.png\n",
      "  - freq_weighted_f1_bar_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_fuzz.png\n",
      "  - model_accuracy_summary.csv\n",
      "  - model_frequency_weighted_f1_summary.csv\n",
      "  - model_performance_summary.csv\n",
      "\n",
      "Summary saved to: /home/jeremias/projects/delphi-explanations/results/visualizations/model_frequency_weighted_f1_summary.csv\n",
      "\n",
      "Best Performing Models by Category:\n",
      "==================================================\n",
      "Highest Frequency-Weighted F1: Qwen3-32B (0.719)\n",
      "Highest Accuracy: Transluce-Llama-8B (Qwen 32B Scorer) (0.846)\n",
      "Highest Precision: Transluce-Llama-8B (Qwen 32B Scorer) (0.898)\n",
      "Highest Recall: Gemma-3-27B-IT (0.856)\n",
      "\n",
      "All visualizations and summaries saved to: /home/jeremias/projects/delphi-explanations/results/visualizations\n",
      "\n",
      "Generated files:\n",
      "  - accuracy_bar_detection.pdf\n",
      "  - accuracy_bar_detection.png\n",
      "  - accuracy_bar_fuzz.pdf\n",
      "  - accuracy_bar_fuzz.png\n",
      "  - accuracy_density_detection.pdf\n",
      "  - accuracy_density_detection.png\n",
      "  - accuracy_density_fuzz.pdf\n",
      "  - accuracy_density_fuzz.png\n",
      "  - accuracy_density_line_detection.pdf\n",
      "  - accuracy_density_line_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_detection.pdf\n",
      "  - freq_weighted_f1_bar_detection.png\n",
      "  - freq_weighted_f1_bar_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_fuzz.png\n",
      "  - model_accuracy_summary.csv\n",
      "  - model_frequency_weighted_f1_summary.csv\n",
      "  - model_performance_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive performance summary using data from get_agg_metrics\n",
    "print(\"Creating comprehensive performance summary...\")\n",
    "\n",
    "# Create frequency-weighted F1 score summary using processed data\n",
    "summary_rows = []\n",
    "for model_name, model_data in model_results.items():\n",
    "    processed_df = model_data['processed_df']\n",
    "    # Use the frequency-weighted F1 from the processed data (averaged across score types)\n",
    "    freq_weighted_f1 = processed_df['weighted_f1'].mean() if 'weighted_f1' in processed_df.columns else None\n",
    "    summary_rows.append({\n",
    "        'model': model_name,\n",
    "        'frequency_weighted_f1': freq_weighted_f1,\n",
    "        'accuracy': processed_df['accuracy'].mean(),\n",
    "        'precision': processed_df['precision'].mean(),\n",
    "        'recall': processed_df['recall'].mean()\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).set_index('model').round(3)\n",
    "\n",
    "print(\"\\nModel Performance Summary (Frequency-Weighted F1 Focus):\")\n",
    "print(\"=\" * 60)\n",
    "print(summary_df.sort_values('frequency_weighted_f1', ascending=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_file = visualizations_dir / \"model_frequency_weighted_f1_summary.csv\"\n",
    "summary_df.to_csv(summary_file)\n",
    "print(f\"\\nSummary saved to: {summary_file}\")\n",
    "\n",
    "# Best performing models by category\n",
    "print(\"\\nBest Performing Models by Category:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Highest Frequency-Weighted F1: {summary_df['frequency_weighted_f1'].idxmax()} ({summary_df['frequency_weighted_f1'].max():.3f})\")\n",
    "print(f\"Highest Accuracy: {summary_df['accuracy'].idxmax()} ({summary_df['accuracy'].max():.3f})\")\n",
    "print(f\"Highest Precision: {summary_df['precision'].idxmax()} ({summary_df['precision'].max():.3f})\")\n",
    "print(f\"Highest Recall: {summary_df['recall'].idxmax()} ({summary_df['recall'].max():.3f})\")\n",
    "\n",
    "print(f\"\\nAll visualizations and summaries saved to: {visualizations_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for file in sorted(visualizations_dir.glob(\"*\")):\n",
    "    print(f\"  - {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
