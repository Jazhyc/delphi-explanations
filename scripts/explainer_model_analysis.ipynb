{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1096fab",
   "metadata": {},
   "source": [
    "# Explainer Model Performance Analysis\n",
    "\n",
    "This notebook analyzes the performance of different explainer models in generating explanations for SAE latent features. We compare models across multiple metrics including accuracy, F1 scores, token usage, and execution time.\n",
    "\n",
    "## Recent Updates\n",
    "- **Updated to 400 latents**: Changed from 100 to 400 latents to match the experiment configuration\n",
    "- **Fixed directory pattern matching**: Now handles both old (with suffix) and new (without suffix) directory naming conventions\n",
    "- **Simplified loading process**: Now uses functions from `result_analysis.py` to reduce redundancy\n",
    "- **Enhanced model support**: Updated model name mapping for better display names and added GPT-OSS-20B\n",
    "- **Bootstrapped confidence intervals**: Error bars now use 95% CI via bootstrapping (2000 resamples) instead of standard deviation\n",
    "- **Progress bars**: Added TQDM progress bars for bootstrap calculations to show progress\n",
    "- **Better variable naming**: Renamed `accuracy_df` to `performance_df` for clarity\n",
    "\n",
    "## Analysis Overview\n",
    "\n",
    "- **Accuracy Distribution**: Density plots showing accuracy distribution for each model\n",
    "- **Mean Performance**: Bar charts comparing mean accuracy across models\n",
    "- **Token Usage**: Analysis of computational efficiency and resource consumption\n",
    "- **Performance Summary**: Comprehensive comparison tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60643f3",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required libraries, define model name mapping for prettier display names, and set up output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prefix: pythiaST\n",
      "Latents count: 400\n",
      "Compare thinking modes: False\n",
      "Results directory: /home/jeremias/projects/delphi-explanations/results\n",
      "Visualizations output: /home/jeremias/projects/delphi-explanations/results/visualizations\n",
      "Experiments directory: /home/jeremias/projects/delphi-explanations/results/pythiaST/400latents\n",
      "Available experiments (400 latents):\n",
      "  - pythiaST_Llama_3_3_70B_Instruct_quantized_w4a16\n",
      "  - pythiaST_Llama_4_Scout_17B_16E_Instruct_quantized_w4a16\n",
      "  - pythiaST_Meta_Llama_3_1_8B_Instruct_GPTQ_INT4\n",
      "  - pythiaST_Qwen3_14B_quantized_w4a16\n",
      "  - pythiaST_Qwen3_32B_quantized_w4a16\n",
      "  - pythiaST_Qwen3_4B_quantized_w4a16\n",
      "  - pythiaST_gemma_3_12b_it_quantized_w4a16\n",
      "  - pythiaST_gemma_3_27b_it_quantized_w4a16\n",
      "  - pythiaST_gemma_3_4b_it_quantized_w4a16\n",
      "  - pythiaST_gpt_oss_20b\n",
      "  - pythiaST_llama_8b_explainer\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add the parent directory to the path to import delphi modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from delphi.log.result_analysis import (\n",
    "    import_plotly,\n",
    "    load_data,\n",
    "    get_agg_metrics,\n",
    "    add_latent_f1,\n",
    "    compute_confusion,\n",
    "    compute_classification_metrics,\n",
    "    frequency_weighted_f1\n",
    ")\n",
    "\n",
    "# Import plotly for plotting\n",
    "px = import_plotly()\n",
    "\n",
    "# Toggle for whether to run expensive bootstrap resampling\n",
    "ENABLE_BOOTSTRAP = True  # Set to True to run bootstrap resampling (slow)\n",
    "BOOTSTRAP_SAMPLES = 100  # Number of bootstrap resamples when enabled\n",
    "N_JOBS = min(4, max(1, (os.cpu_count() or 1) - 1))  # Conservative parallelism for stability\n",
    "# Confidence level for CI calculations (e.g. 0.95 for 95% CI). Make configurable.\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "# Configuration - Model prefix for experiment naming\n",
    "MODEL_PREFIX = \"pythiaST\"  # Change this to match your base model (e.g., \"pythia\", \"gemma\", etc.)\n",
    "LATENTS_COUNT = 400  # Number of latents used in experiments\n",
    "COMPARE_THINKING_MODES = False  # Set to True to compare both thinking and non-thinking modes (Currently broken if set to True)\n",
    "\n",
    "# Define model name mapping for prettier display - updated to match actual directory names\n",
    "MODEL_NAME_MAPPING = {\n",
    "    \"gemma_3_4b_it_quantized_w4a16\": \"Gemma-3-4B-IT\",\n",
    "    \"Qwen3_4B_quantized_w4a16\": \"Qwen3-4B\", \n",
    "    \"gemma_3_12b_it_quantized_w4a16\": \"Gemma-3-12B-IT\",\n",
    "    \"gemma_3_27b_it_quantized_w4a16\": \"Gemma-3-27B-IT\",\n",
    "    \"Qwen3_14B_quantized_w4a16\": \"Qwen3-14B\",\n",
    "    \"Qwen3_32B_quantized_w4a16\": \"Qwen3-32B\",\n",
    "    \"Qwen3_235B_A22B_GPTQ_Int4\": \"Qwen3-235B\",\n",
    "    \"Llama_3_3_70B_Instruct_quantized_w4a16\": \"Llama-3.3-70B-Instruct\",\n",
    "    \"Llama_3_1_70B_Instruct_NVFP4\": \"Llama-3.1-70B-Instruct\",\n",
    "    \"Llama_4_Scout_17B_16E_Instruct_quantized_w4a16\": \"Llama-4-Scout-17B\",\n",
    "    \"Llama_4_Maverick_17B_128E_Instruct_quantized_w4a16\": \"Llama-4-Maverick-17B\",\n",
    "    \"llama_8b_explainer\": \"Transluce-Llama-8B (Qwen 32B Scorer)\",\n",
    "    \"Meta_Llama_3_1_8B_Instruct_GPTQ_INT4\": \"Llama-8B (Qwen 32B Scorer)\",\n",
    "    \"gpt_oss_20b\": \"GPT-OSS-20B\"\n",
    "}\n",
    "\n",
    "def load_model_results(results_dir: Path, model_mapping: dict, model_prefix: str, latents_count: int, compare_modes: bool = False):\n",
    "    \"\"\"Load results and statistics for all explainer models using functions from result_analysis.py.\"\"\"\n",
    "    model_results = {}\n",
    "    model_stats = {}\n",
    "\n",
    "    # Base directory for experiments with this latent count\n",
    "    experiments_base = results_dir / model_prefix / f\"{latents_count}latents\"\n",
    "\n",
    "    if not experiments_base.exists():\n",
    "        print(f\"Warning: Experiments directory not found: {experiments_base}\")\n",
    "        return model_results, model_stats\n",
    "\n",
    "    # Pattern for experiment directories (exclude thinking mode and hide directories)\n",
    "    # Handle both naming conventions: with and without latents suffix\n",
    "    pattern_with_suffix = f\"{model_prefix}_*_{latents_count}latents\"\n",
    "    pattern_without_suffix = f\"{model_prefix}_*\"\n",
    "    \n",
    "    all_exp_dirs = []\n",
    "    # First try directories with latents suffix (older format)\n",
    "    for exp_dir in experiments_base.glob(pattern_with_suffix):\n",
    "        if not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name):\n",
    "            all_exp_dirs.append(exp_dir)\n",
    "    \n",
    "    # Then try directories without latents suffix (newer format)\n",
    "    if not all_exp_dirs:\n",
    "        for exp_dir in experiments_base.glob(pattern_without_suffix):\n",
    "            if (not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name) and \n",
    "                exp_dir.name != \"cache\"):  # Exclude cache directory\n",
    "                all_exp_dirs.append(exp_dir)\n",
    "\n",
    "    for exp_dir in all_exp_dirs:\n",
    "        # Skip directories with \"thinking\" or \"hide\" in the name\n",
    "        if \"thinking\" in exp_dir.name or \"hide\" in exp_dir.name:\n",
    "            continue\n",
    "\n",
    "        # Extract model name from directory: handle both naming conventions\n",
    "        dir_name = exp_dir.name\n",
    "        if dir_name.endswith(f\"_{latents_count}latents\"):\n",
    "            # Old format: pythiaST_model_400latents\n",
    "            model_key = dir_name.replace(f\"{model_prefix}_\", \"\").replace(f\"_{latents_count}latents\", \"\")\n",
    "        else:\n",
    "            # New format: pythiaST_model\n",
    "            model_key = dir_name.replace(f\"{model_prefix}_\", \"\")\n",
    "        \n",
    "        base_display_name = model_mapping.get(model_key, model_key)\n",
    "        display_name = base_display_name\n",
    "\n",
    "        scores_path = exp_dir / \"scores\"\n",
    "        if scores_path.exists():\n",
    "            # Load scoring results using functions from result_analysis.py\n",
    "            try:\n",
    "                # All experiments now use the shared cache\n",
    "                cache_dir_name = \"cache\"\n",
    "                latents_path = experiments_base.parent / cache_dir_name / \"latents\"\n",
    "                if not latents_path.exists():\n",
    "                    latents_path = exp_dir / \"latents\"  # Fallback to local latents\n",
    "\n",
    "                if latents_path.exists():\n",
    "                    # Extract module names from the actual files\n",
    "                    sample_score_dir = next(scores_path.iterdir())\n",
    "                    sample_files = list(sample_score_dir.glob(\"*.txt\"))\n",
    "                    if sample_files:\n",
    "                        # Extract module name from filename pattern (e.g., \"layers.32_latent0.txt\" -> \"layers.32\")\n",
    "                        sample_filename = sample_files[0].stem\n",
    "                        module_name = sample_filename.split('_latent')[0]\n",
    "                        modules = [module_name]\n",
    "                    else:\n",
    "                        print(f\"No score files found in {sample_score_dir}\")\n",
    "                        continue\n",
    "\n",
    "                    # Use load_data from result_analysis.py\n",
    "                    latent_df, counts = load_data(scores_path, latents_path, modules)\n",
    "\n",
    "                    if latent_df.empty:\n",
    "                        print(f\"No data found for {display_name}\")\n",
    "                        continue\n",
    "\n",
    "                    # Use add_latent_f1 and get_agg_metrics from result_analysis.py\n",
    "                    latent_df = add_latent_f1(latent_df)\n",
    "                    processed_df = get_agg_metrics(latent_df, counts)\n",
    "\n",
    "                    model_results[display_name] = {\n",
    "                        'latent_df': latent_df,\n",
    "                        'processed_df': processed_df,\n",
    "                        'counts': counts\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"Latents path not found for {display_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading results for {model_key}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Load explainer statistics\n",
    "        stats_file = exp_dir / \"explainer_stats.json\"\n",
    "        if stats_file.exists():\n",
    "            try:\n",
    "                with open(stats_file, 'r') as f:\n",
    "                    stats = json.load(f)\n",
    "                    model_stats[display_name] = stats\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading stats for {model_key}: {e}\")\n",
    "                model_stats[display_name] = None\n",
    "        else:\n",
    "            model_stats[display_name] = None\n",
    "\n",
    "    return model_results, model_stats\n",
    "\n",
    "# Set up directories\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "visualizations_dir = results_dir / \"visualizations\"\n",
    "visualizations_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Model prefix: {MODEL_PREFIX}\")\n",
    "print(f\"Latents count: {LATENTS_COUNT}\")\n",
    "print(f\"Compare thinking modes: {COMPARE_THINKING_MODES}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Visualizations output: {visualizations_dir}\")\n",
    "\n",
    "# Check if the experiments directory exists\n",
    "experiments_dir = results_dir / MODEL_PREFIX / f\"{LATENTS_COUNT}latents\"\n",
    "print(f\"Experiments directory: {experiments_dir}\")\n",
    "\n",
    "if experiments_dir.exists():\n",
    "    print(f\"Available experiments ({LATENTS_COUNT} latents):\")\n",
    "    # Handle both naming conventions: with and without latents suffix\n",
    "    pattern_with_suffix = f\"{MODEL_PREFIX}_*_{LATENTS_COUNT}latents\"\n",
    "    pattern_without_suffix = f\"{MODEL_PREFIX}_*\"\n",
    "    \n",
    "    all_exp_dirs = []\n",
    "    # First try directories with latents suffix (older format)\n",
    "    for exp_dir in sorted(experiments_dir.glob(pattern_with_suffix)):\n",
    "        if not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name):\n",
    "            all_exp_dirs.append(exp_dir)\n",
    "    \n",
    "    # Then try directories without latents suffix (newer format)\n",
    "    if not all_exp_dirs:\n",
    "        for exp_dir in sorted(experiments_dir.glob(pattern_without_suffix)):\n",
    "            if (not (\"thinking\" in exp_dir.name or \"hide\" in exp_dir.name) and \n",
    "                exp_dir.name != \"cache\"):  # Exclude cache directory\n",
    "                all_exp_dirs.append(exp_dir)\n",
    "    \n",
    "    for exp_dir in all_exp_dirs:\n",
    "        print(f\"  - {exp_dir.name}\")\n",
    "else:\n",
    "    print(f\"Warning: Experiments directory not found: {experiments_dir}\")\n",
    "    print(\"Make sure experiments have been run with the specified latent count.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a672b",
   "metadata": {},
   "source": [
    "## 2. Load Model Results\n",
    "\n",
    "Load explanation comparison results from all models and extract performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44990708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "/home/jeremias/projects/delphi-explanations/delphi/log/result_analysis.py:444: FutureWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded results for 11 models:\n",
      "  - Qwen3-4B\n",
      "  - Gemma-3-4B-IT\n",
      "  - Gemma-3-12B-IT\n",
      "  - Gemma-3-27B-IT\n",
      "  - Qwen3-14B\n",
      "  - Qwen3-32B\n",
      "  - Llama-3.3-70B-Instruct\n",
      "  - GPT-OSS-20B\n",
      "  - Llama-8B (Qwen 32B Scorer)\n",
      "  - Llama-4-Scout-17B\n",
      "  - Transluce-Llama-8B (Qwen 32B Scorer)\n",
      "\n",
      "Token usage statistics available for 11 models:\n",
      "  - Qwen3-4B: ['DefaultExplainer']\n",
      "  - Gemma-3-4B-IT: ['DefaultExplainer']\n",
      "  - Gemma-3-12B-IT: ['DefaultExplainer']\n",
      "  - Gemma-3-27B-IT: ['DefaultExplainer']\n",
      "  - Qwen3-14B: ['DefaultExplainer']\n",
      "  - Qwen3-32B: ['DefaultExplainer']\n",
      "  - Llama-3.3-70B-Instruct: ['DefaultExplainer']\n",
      "  - GPT-OSS-20B: ['DefaultExplainer']\n",
      "  - Llama-8B (Qwen 32B Scorer): ['DefaultExplainer']\n",
      "  - Llama-4-Scout-17B: ['DefaultExplainer']\n",
      "  - Transluce-Llama-8B (Qwen 32B Scorer): ['DefaultExplainer']\n",
      "\n",
      "Sample metrics from Qwen3-4B:\n",
      "  score_type  accuracy  f1_score  precision  recall\n",
      "0  detection     0.757     0.755      0.763   0.747\n",
      "1       fuzz     0.818     0.827      0.787   0.871\n"
     ]
    }
   ],
   "source": [
    "# Load all model results and statistics\n",
    "print(\"Loading model results...\")\n",
    "model_results, model_stats = load_model_results(results_dir, MODEL_NAME_MAPPING, MODEL_PREFIX, LATENTS_COUNT, COMPARE_THINKING_MODES)\n",
    "\n",
    "print(f\"\\nLoaded results for {len(model_results)} models:\")\n",
    "for model_name in model_results.keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "print(f\"\\nToken usage statistics available for {len([k for k, v in model_stats.items() if v is not None])} models:\")\n",
    "for model_name, stats in model_stats.items():\n",
    "    if stats:\n",
    "        print(f\"  - {model_name}: {list(stats.keys())}\")\n",
    "    else:\n",
    "        print(f\"  - {model_name}: No stats available\")\n",
    "\n",
    "# Display sample metrics for the first model\n",
    "if model_results:\n",
    "    sample_model = list(model_results.keys())[0]\n",
    "    sample_data = model_results[sample_model]['processed_df']\n",
    "    print(f\"\\nSample metrics from {sample_model}:\")\n",
    "    print(sample_data[['score_type', 'accuracy', 'f1_score', 'precision', 'recall']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a674b50",
   "metadata": {},
   "source": [
    "## 3. Generate Mean F1 Score Bar Charts\n",
    "\n",
    "Create bar charts displaying mean F1 scores with **bootstrapped 95% confidence intervals** (2000 resamples) for each model and score type. This provides more robust uncertainty estimates compared to standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0916e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing frequency-weighted F1 score bar charts (bootstrapping=True)...\n",
      "Computing error bars for score type: detection (bootstrap=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7965a30aa024d7895c99a75aad4c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bootstrapping (2000):   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 149\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m st \u001b[38;5;129;01min\u001b[39;00m all_score_types:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing error bars for score type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (bootstrap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENABLE_BOOTSTRAP\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     error_bar_tables[st] = \u001b[43mcompute_error_bars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mENABLE_BOOTSTRAP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBOOTSTRAP_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIDENCE_LEVEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# Now create bar charts using precomputed error bars\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score_type, error_df \u001b[38;5;129;01min\u001b[39;00m error_bar_tables.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mcompute_error_bars\u001b[39m\u001b[34m(model_results, score_type, enable_bootstrap, n_boot, confidence)\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enable_bootstrap:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     samples = \u001b[43mrun_bootstrap_single_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     ci_lower_error, ci_upper_error = compute_ci_from_samples(samples, freq_weighted_f1, confidence)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Fast path: compute weighted percentiles across per-latent F1s using firing_count as weight\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mrun_bootstrap_single_thread\u001b[39m\u001b[34m(score_subset, counts, n_boot, confidence)\u001b[39m\n\u001b[32m     82\u001b[39m     boot_idx = idxs[i]\n\u001b[32m     83\u001b[39m boot_subset = score_subset.iloc[boot_idx]\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m per_module_vals = \u001b[43mcompute_per_module_f1s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboot_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m per_module_vals:\n\u001b[32m     86\u001b[39m     samples.append(\u001b[38;5;28mfloat\u001b[39m(np.mean(per_module_vals)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mcompute_per_module_f1s\u001b[39m\u001b[34m(score_subset, counts)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (module, latent_idx), grp \u001b[38;5;129;01min\u001b[39;00m score_subset.groupby([\u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlatent_idx\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m counts \u001b[38;5;129;01mand\u001b[39;00m latent_idx < \u001b[38;5;28mlen\u001b[39m(counts[module]):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         f1 = compute_classification_metrics(\u001b[43mcompute_confusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrp\u001b[49m\u001b[43m)\u001b[49m)[\u001b[33m\"\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     14\u001b[39m         fire = counts[module][latent_idx].item()\n\u001b[32m     15\u001b[39m         rows.append({\u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m: module, \u001b[33m\"\u001b[39m\u001b[33mlatent_idx\u001b[39m\u001b[33m\"\u001b[39m: latent_idx, \u001b[33m\"\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m\"\u001b[39m: f1, \u001b[33m\"\u001b[39m\u001b[33mfiring_count\u001b[39m\u001b[33m\"\u001b[39m: fire})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/delphi/log/result_analysis.py:260\u001b[39m, in \u001b[36mcompute_confusion\u001b[39m\u001b[34m(df, threshold)\u001b[39m\n\u001b[32m    257\u001b[39m neg = total - pos\n\u001b[32m    259\u001b[39m tp = ((df_valid.prediction >= threshold) & act).sum()\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m tn = ((\u001b[43mdf_valid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m) & ~act).sum()\n\u001b[32m    261\u001b[39m fp = ((df_valid.prediction >= threshold) & ~act).sum()\n\u001b[32m    262\u001b[39m fn = ((df_valid.prediction < threshold) & act).sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/arraylike.py:48\u001b[39m, in \u001b[36mOpsMixin.__lt__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__lt__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__lt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/series.py:6132\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6128\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   6130\u001b[39m res_values = ops.comparison_op(lvalues, rvalues, op)\n\u001b[32m-> \u001b[39m\u001b[32m6132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/series.py:6242\u001b[39m, in \u001b[36mSeries._construct_result\u001b[39m\u001b[34m(self, result, name)\u001b[39m\n\u001b[32m   6239\u001b[39m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[32m   6240\u001b[39m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[32m   6241\u001b[39m dtype = \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6242\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   6243\u001b[39m out = out.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   6245\u001b[39m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[32m   6246\u001b[39m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/series.py:584\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    582\u001b[39m         data = data.copy()\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     data = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    587\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/construction.py:551\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    548\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    550\u001b[39m object_index = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ABCIndex) \u001b[38;5;129;01mand\u001b[39;00m data.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    552\u001b[39m     object_index = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# extract ndarray or ExtensionArray, ensure we have no NumpyExtensionArray\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/delphi-explanations/.venv/lib/python3.11/site-packages/pandas/core/dtypes/generic.py:42\u001b[39m, in \u001b[36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[39m\u001b[34m(cls, inst)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(inst, attr, \u001b[33m\"\u001b[39m\u001b[33m_typ\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m comp\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# error: 'classmethod' used with a non-method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _check(inst) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_subclasscheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Raise instead of returning False\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# This is consistent with default __subclasscheck__ behavior\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generate inline bar charts for frequency-weighted F1 scores with optional bootstrapped CI error bars and save to files\n",
    "print(\"Preparing frequency-weighted F1 score bar charts (bootstrapping={})...\".format(ENABLE_BOOTSTRAP))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Helper: per-module F1s (unchanged)\n",
    "def compute_per_module_f1s(score_subset, counts):\n",
    "    \"\"\"Return list of per-module frequency-weighted F1 values computed from the given score_subset (no resampling).\n",
    "    Each module contributes a single weighted F1 computed from its latents in score_subset.\"\"\"\n",
    "    rows = []\n",
    "    for (module, latent_idx), grp in score_subset.groupby([\"module\", \"latent_idx\"]):\n",
    "        if module in counts and latent_idx < len(counts[module]):\n",
    "            f1 = compute_classification_metrics(compute_confusion(grp))[\"f1_score\"]\n",
    "            fire = counts[module][latent_idx].item()\n",
    "            rows.append({\"module\": module, \"latent_idx\": latent_idx, \"f1_score\": f1, \"firing_count\": fire})\n",
    "    if not rows:\n",
    "        return []\n",
    "    df = pd.DataFrame(rows)\n",
    "    per_module_vals = []\n",
    "    for module in df['module'].unique():\n",
    "        mdf = df[df['module'] == module]\n",
    "        firing_weights = counts[module][mdf['latent_idx']].float()\n",
    "        total_weight = firing_weights.sum()\n",
    "        if total_weight > 0:\n",
    "            f1_tensor = torch.as_tensor(mdf['f1_score'].values, dtype=torch.float32)\n",
    "            module_f1 = (f1_tensor * firing_weights).sum() / firing_weights.sum()\n",
    "            per_module_vals.append(module_f1.item())\n",
    "    return per_module_vals\n",
    "\n",
    "# Helper: weighted percentile (unchanged)\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    \"\"\"Compute weighted percentiles for 1D arrays of values and corresponding non-negative weights.\"\"\"\n",
    "    if len(values) == 0:\n",
    "        return [np.nan for _ in np.atleast_1d(percentiles)]\n",
    "    vals = np.asarray(values, dtype=float)\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    if w.sum() == 0:\n",
    "        # Fall back to unweighted percentiles\n",
    "        return np.percentile(vals, percentiles)\n",
    "    # sort by value\n",
    "    order = np.argsort(vals)\n",
    "    vals = vals[order]\n",
    "    w = w[order]\n",
    "    cumsum = np.cumsum(w)\n",
    "    total = cumsum[-1]\n",
    "    # cumulative fraction positions of each value\n",
    "    cumfrac = (cumsum - 0.5 * w) / total\n",
    "    out = []\n",
    "    for p in np.atleast_1d(percentiles) / 100.0:\n",
    "        # find the first index where cumfrac >= p\n",
    "        idx = np.searchsorted(cumfrac, p, side='left')\n",
    "        if idx >= len(vals):\n",
    "            out.append(vals[-1])\n",
    "        else:\n",
    "            out.append(vals[idx])\n",
    "    return out if len(out) > 1 else out[0]\n",
    "\n",
    "# Helper: compute CI errors from samples\n",
    "def compute_ci_from_samples(samples, freq_weighted_f1, confidence):\n",
    "    if not samples:\n",
    "        return 0.0, 0.0\n",
    "    lower_pct = (1.0 - confidence) / 2.0 * 100.0\n",
    "    upper_pct = (1.0 + confidence) / 2.0 * 100.0\n",
    "    lower_ci, upper_ci = np.percentile(samples, [lower_pct, upper_pct])\n",
    "    return float(freq_weighted_f1 - lower_ci), float(upper_ci - freq_weighted_f1)\n",
    "\n",
    "# Helper: single-threaded bootstrap that precomputes indices for speed\n",
    "def run_bootstrap_single_thread(score_subset, counts, n_boot, confidence):\n",
    "    n = len(score_subset)\n",
    "    samples = []\n",
    "    if n <= 0:\n",
    "        return samples\n",
    "    # Try to precompute indices for speed (may be memory heavy for very large n_boot * n)\n",
    "    try:\n",
    "        idxs = np.random.randint(0, n, size=(n_boot, n))\n",
    "    except Exception:\n",
    "        idxs = None\n",
    "    for i in tqdm(range(n_boot), desc=f\"Bootstrapping ({n_boot})\", leave=False):\n",
    "        if idxs is None:\n",
    "            boot_idx = np.random.randint(0, n, size=n)\n",
    "        else:\n",
    "            boot_idx = idxs[i]\n",
    "        boot_subset = score_subset.iloc[boot_idx]\n",
    "        per_module_vals = compute_per_module_f1s(boot_subset, counts)\n",
    "        if per_module_vals:\n",
    "            samples.append(float(np.mean(per_module_vals)))\n",
    "    return samples\n",
    "\n",
    "# Main: compute_error_bars simplified to single-threaded bootstrap and modular helpers\n",
    "def compute_error_bars(model_results, score_type, enable_bootstrap: bool = ENABLE_BOOTSTRAP, n_boot: int = BOOTSTRAP_SAMPLES, confidence: float = CONFIDENCE_LEVEL):\n",
    "    \"\"\"Compute frequency-weighted F1 and CI errors for every model for a given score_type.\n",
    "\n",
    "    Returns a DataFrame with columns: model, frequency_weighted_f1, ci_lower_error, ci_upper_error\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model_name, data in model_results.items():\n",
    "        processed_df = data['processed_df']\n",
    "        score_row = processed_df[processed_df['score_type'] == score_type]\n",
    "        if len(score_row) == 0:\n",
    "            continue\n",
    "        freq_weighted_f1 = score_row['weighted_f1'].iloc[0] if 'weighted_f1' in score_row.columns else None\n",
    "        latent_df = data['latent_df']\n",
    "        counts = data['counts']\n",
    "        score_subset = latent_df[latent_df['score_type'] == score_type]\n",
    "        if len(score_subset) == 0 or counts is None or freq_weighted_f1 is None:\n",
    "            rows.append({'model': model_name, 'frequency_weighted_f1': freq_weighted_f1, 'ci_lower_error': 0.0, 'ci_upper_error': 0.0})\n",
    "            continue\n",
    "\n",
    "        if enable_bootstrap:\n",
    "            samples = run_bootstrap_single_thread(score_subset, counts, n_boot, confidence)\n",
    "            ci_lower_error, ci_upper_error = compute_ci_from_samples(samples, freq_weighted_f1, confidence)\n",
    "        else:\n",
    "            # Fast path: compute weighted percentiles across per-latent F1s using firing_count as weight\n",
    "            vals = []\n",
    "            wts = []\n",
    "            for (module, latent_idx), grp in score_subset.groupby([\"module\", \"latent_idx\"]):\n",
    "                if module in counts and latent_idx < len(counts[module]):\n",
    "                    f1 = compute_classification_metrics(compute_confusion(grp))[\"f1_score\"]\n",
    "                    fire = counts[module][latent_idx].item()\n",
    "                    vals.append(float(f1))\n",
    "                    wts.append(float(fire))\n",
    "            if len(vals) == 0:\n",
    "                ci_lower_error = 0.0\n",
    "                ci_upper_error = 0.0\n",
    "            else:\n",
    "                lower_pct = (1.0 - confidence) / 2.0 * 100.0\n",
    "                upper_pct = (1.0 + confidence) / 2.0 * 100.0\n",
    "                lower_ci, upper_ci = weighted_percentile(np.array(vals), np.array(wts), [lower_pct, upper_pct])\n",
    "                lower_ci = float(lower_ci)\n",
    "                upper_ci = float(upper_ci)\n",
    "                ci_lower_error = float(freq_weighted_f1 - lower_ci)\n",
    "                ci_upper_error = float(upper_ci - freq_weighted_f1)\n",
    "\n",
    "        rows.append({'model': model_name, 'frequency_weighted_f1': float(freq_weighted_f1), 'ci_lower_error': ci_lower_error, 'ci_upper_error': ci_upper_error})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example: compute error bars for each score_type and save/use for plotting\n",
    "all_score_types = set()\n",
    "for data in model_results.values():\n",
    "    all_score_types.update(list(data['processed_df']['score_type'].unique()))\n",
    "all_score_types = sorted(list(all_score_types))\n",
    "\n",
    "# Precompute error bar tables for reuse in other plots\n",
    "error_bar_tables = {}\n",
    "for st in all_score_types:\n",
    "    print(f\"Computing error bars for score type: {st} (bootstrap={ENABLE_BOOTSTRAP})\")\n",
    "    error_bar_tables[st] = compute_error_bars(model_results, st, enable_bootstrap=ENABLE_BOOTSTRAP, n_boot=BOOTSTRAP_SAMPLES, confidence=CONFIDENCE_LEVEL)\n",
    "\n",
    "# Now create bar charts using precomputed error bars\n",
    "for score_type, error_df in error_bar_tables.items():\n",
    "    if error_df.empty:\n",
    "        continue\n",
    "    error_df = error_df.sort_values('frequency_weighted_f1', ascending=False)\n",
    "    fig = px.bar(\n",
    "        error_df,\n",
    "        x='model',\n",
    "        y='frequency_weighted_f1',\n",
    "        title=f'Frequency-Weighted F1 Score by Model - {score_type.title()} ({int(CONFIDENCE_LEVEL*100)}% CI)',\n",
    "        text='frequency_weighted_f1'\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=error_df['ci_upper_error'],\n",
    "            arrayminus=error_df['ci_lower_error']\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis_range=[0, 1],\n",
    "        xaxis_title=\"Model\",\n",
    "        yaxis_title=f\"Frequency-Weighted F1 Score ({int(CONFIDENCE_LEVEL*100)}% CI)\",\n",
    "        xaxis={'tickangle': 45},\n",
    "        height=500\n",
    "    )\n",
    "    fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "    fig.show()\n",
    "    output_file_pdf = visualizations_dir / f\"freq_weighted_f1_bar_{score_type}.pdf\"\n",
    "    output_file_png = visualizations_dir / f\"freq_weighted_f1_bar_{score_type}.png\"\n",
    "    fig.write_image(str(output_file_pdf))\n",
    "    fig.write_image(str(output_file_png))\n",
    "    print(f\"Saved Frequency-Weighted F1 bar chart: {output_file_pdf}\")\n",
    "\n",
    "print(\"\\nBar charts saved to\", visualizations_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1c475",
   "metadata": {},
   "source": [
    "## 5. Performance Summary\n",
    "\n",
    "Display a summary table of F1 scores for each model and score type, including mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive performance summary...\n",
      "\n",
      "Model Performance Summary (Frequency-Weighted F1 Focus):\n",
      "============================================================\n",
      "                                      frequency_weighted_f1  accuracy  \\\n",
      "model                                                                   \n",
      "Qwen3-32B                                             0.719     0.845   \n",
      "Gemma-3-27B-IT                                        0.715     0.833   \n",
      "Qwen3-4B                                              0.686     0.787   \n",
      "Llama-4-Scout-17B                                     0.674     0.813   \n",
      "Transluce-Llama-8B (Qwen 32B Scorer)                  0.657     0.838   \n",
      "Llama-3.3-70B-Instruct                                0.651     0.842   \n",
      "Gemma-3-12B-IT                                        0.642     0.788   \n",
      "Qwen3-14B                                             0.627     0.813   \n",
      "Llama-8B (Qwen 32B Scorer)                            0.607     0.775   \n",
      "Gemma-3-4B-IT                                         0.596     0.584   \n",
      "GPT-OSS-20B                                           0.372     0.669   \n",
      "\n",
      "                                      precision  recall  \n",
      "model                                                    \n",
      "Qwen3-32B                                 0.855   0.831  \n",
      "Gemma-3-27B-IT                            0.817   0.856  \n",
      "Qwen3-4B                                  0.775   0.809  \n",
      "Llama-4-Scout-17B                         0.826   0.793  \n",
      "Transluce-Llama-8B (Qwen 32B Scorer)      0.890   0.772  \n",
      "Llama-3.3-70B-Instruct                    0.870   0.806  \n",
      "Gemma-3-12B-IT                            0.816   0.750  \n",
      "Qwen3-14B                                 0.877   0.731  \n",
      "Llama-8B (Qwen 32B Scorer)                0.808   0.735  \n",
      "Gemma-3-4B-IT                             0.567   0.733  \n",
      "GPT-OSS-20B                               0.656   0.755  \n",
      "\n",
      "Summary saved to: /home/jeremias/projects/delphi-explanations/results/visualizations/model_frequency_weighted_f1_summary.csv\n",
      "\n",
      "Best Performing Models by Category:\n",
      "==================================================\n",
      "Highest Frequency-Weighted F1: Qwen3-32B (0.719)\n",
      "Highest Accuracy: Qwen3-32B (0.845)\n",
      "Highest Precision: Transluce-Llama-8B (Qwen 32B Scorer) (0.890)\n",
      "Highest Recall: Gemma-3-27B-IT (0.856)\n",
      "\n",
      "All visualizations and summaries saved to: /home/jeremias/projects/delphi-explanations/results/visualizations\n",
      "\n",
      "Generated files:\n",
      "  - accuracy_bar_detection.pdf\n",
      "  - accuracy_bar_detection.png\n",
      "  - accuracy_bar_fuzz.pdf\n",
      "  - accuracy_bar_fuzz.png\n",
      "  - accuracy_density_detection.pdf\n",
      "  - accuracy_density_detection.png\n",
      "  - accuracy_density_fuzz.pdf\n",
      "  - accuracy_density_fuzz.png\n",
      "  - accuracy_density_line_detection.pdf\n",
      "  - accuracy_density_line_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_detection.pdf\n",
      "  - freq_weighted_f1_bar_detection.png\n",
      "  - freq_weighted_f1_bar_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_fuzz.png\n",
      "  - model_accuracy_summary.csv\n",
      "  - model_frequency_weighted_f1_summary.csv\n",
      "  - model_performance_summary.csv\n",
      "\n",
      "Summary saved to: /home/jeremias/projects/delphi-explanations/results/visualizations/model_frequency_weighted_f1_summary.csv\n",
      "\n",
      "Best Performing Models by Category:\n",
      "==================================================\n",
      "Highest Frequency-Weighted F1: Qwen3-32B (0.719)\n",
      "Highest Accuracy: Qwen3-32B (0.845)\n",
      "Highest Precision: Transluce-Llama-8B (Qwen 32B Scorer) (0.890)\n",
      "Highest Recall: Gemma-3-27B-IT (0.856)\n",
      "\n",
      "All visualizations and summaries saved to: /home/jeremias/projects/delphi-explanations/results/visualizations\n",
      "\n",
      "Generated files:\n",
      "  - accuracy_bar_detection.pdf\n",
      "  - accuracy_bar_detection.png\n",
      "  - accuracy_bar_fuzz.pdf\n",
      "  - accuracy_bar_fuzz.png\n",
      "  - accuracy_density_detection.pdf\n",
      "  - accuracy_density_detection.png\n",
      "  - accuracy_density_fuzz.pdf\n",
      "  - accuracy_density_fuzz.png\n",
      "  - accuracy_density_line_detection.pdf\n",
      "  - accuracy_density_line_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_detection.pdf\n",
      "  - freq_weighted_f1_bar_detection.png\n",
      "  - freq_weighted_f1_bar_fuzz.pdf\n",
      "  - freq_weighted_f1_bar_fuzz.png\n",
      "  - model_accuracy_summary.csv\n",
      "  - model_frequency_weighted_f1_summary.csv\n",
      "  - model_performance_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive performance summary using data from get_agg_metrics\n",
    "print(\"Creating comprehensive performance summary...\")\n",
    "\n",
    "# Create frequency-weighted F1 score summary using processed data\n",
    "summary_rows = []\n",
    "for model_name, model_data in model_results.items():\n",
    "    processed_df = model_data['processed_df']\n",
    "    # Use the frequency-weighted F1 from the processed data (averaged across score types)\n",
    "    freq_weighted_f1 = processed_df['weighted_f1'].mean() if 'weighted_f1' in processed_df.columns else None\n",
    "    summary_rows.append({\n",
    "        'model': model_name,\n",
    "        'frequency_weighted_f1': freq_weighted_f1,\n",
    "        'accuracy': processed_df['accuracy'].mean(),\n",
    "        'precision': processed_df['precision'].mean(),\n",
    "        'recall': processed_df['recall'].mean()\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).set_index('model').round(3)\n",
    "\n",
    "print(\"\\nModel Performance Summary (Frequency-Weighted F1 Focus):\")\n",
    "print(\"=\" * 60)\n",
    "print(summary_df.sort_values('frequency_weighted_f1', ascending=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_file = visualizations_dir / \"model_frequency_weighted_f1_summary.csv\"\n",
    "summary_df.to_csv(summary_file)\n",
    "print(f\"\\nSummary saved to: {summary_file}\")\n",
    "\n",
    "# Best performing models by category\n",
    "print(\"\\nBest Performing Models by Category:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Highest Frequency-Weighted F1: {summary_df['frequency_weighted_f1'].idxmax()} ({summary_df['frequency_weighted_f1'].max():.3f})\")\n",
    "print(f\"Highest Accuracy: {summary_df['accuracy'].idxmax()} ({summary_df['accuracy'].max():.3f})\")\n",
    "print(f\"Highest Precision: {summary_df['precision'].idxmax()} ({summary_df['precision'].max():.3f})\")\n",
    "print(f\"Highest Recall: {summary_df['recall'].idxmax()} ({summary_df['recall'].max():.3f})\")\n",
    "\n",
    "print(f\"\\nAll visualizations and summaries saved to: {visualizations_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for file in sorted(visualizations_dir.glob(\"*\")):\n",
    "    print(f\"  - {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
